<snippet>
  <content><![CDATA[- name: ${1:Creates a GCP Dataset}
  google.cloud.gcp_bigquery_dataset:
    access: ${2:# An array of objects that define dataset access for one or more entities}
    access_token: ${3:# An OAuth2 access token if credential type is accesstoken}
    auth_kind: ${4:# The type of credential used}
    dataset_reference: ${5:# A reference that identifies the dataset}
    default_encryption_configuration: ${6:# The default encryption key for all tables in the dataset}
    default_partition_expiration_ms: ${7:# The default partition expiration for all partitioned tables in the dataset, in milliseconds}
    default_table_expiration_ms: ${8:# The default lifetime of all tables in the dataset, in milliseconds}
    description: ${9:# A user-friendly description of the dataset}
    env_type: ${10:# Specifies which Ansible environment you're running this module within}
    friendly_name: ${11:# A descriptive name for the dataset}
    labels: ${12:# The labels associated with this dataset}
    location: ${13:# The geographic location where the dataset should reside}
    name: ${14:# Dataset name}
    project: ${15:# The Google Cloud Platform project to use}
    scopes: ${16:# Array of scopes to be used}
    service_account_contents: ${17:# The contents of a Service Account JSON file, either in a dictionary or as a JSON string that represents it}
    service_account_email: ${18:# An optional service account email address if machineaccount is selected and the user does not wish to use the default email}
    service_account_file: ${19:# The path of a Service Account JSON file if serviceaccount is selected as type}
    state: ${20:# Whether the given object should exist in GCP}
  tags:
    - google
    - cloud
    - gcp_bigquery_dataset]]></content>
  <tabTrigger>google.cloud.gcp_bigquery_dataset</tabTrigger>
  <scope>source.yaml,source.ansible</scope>
  <description>Creates a GCP Dataset</description>
</snippet>